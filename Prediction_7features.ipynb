{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",header=None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','salary-class']\n",
    "occ = data['occupation'].value_counts()\n",
    "wor = data['workclass'].value_counts()\n",
    "capacity = wor.sum() #32561\n",
    "\n",
    "# data extractionï¼š6 continuous features\n",
    "# age, education-num, capital-gain, capital-loss, hours-per-week,fnlwgt\n",
    "age = data['age']\n",
    "eduNum = data['education-num']\n",
    "capGain = data['capital-gain']\n",
    "capLoss = data['capital-loss']\n",
    "hpw = data['hours-per-week']\n",
    "fnl = data['fnlwgt']\n",
    "sex = data['sex']\n",
    "salary = data['salary-class']\n",
    "assert ' ?' not in age.value_counts().index  # no missing data\n",
    "assert ' ?' not in eduNum.value_counts().index\n",
    "assert ' ?' not in hpw.value_counts().index\n",
    "assert ' ?' not in capGain.value_counts().index\n",
    "assert ' ?' not in capLoss.value_counts().index\n",
    "assert ' ?' not in fnl.value_counts().index\n",
    "assert ' ?' not in sex.value_counts().index\n",
    "assert ' ?' not in salary.value_counts().index\n",
    "age = age.tolist()\n",
    "eduNum = eduNum.tolist()\n",
    "capGain = capGain.tolist()\n",
    "capLoss = capLoss.tolist()\n",
    "hpw = hpw.tolist()\n",
    "fnl = fnl.tolist()\n",
    "sex = sex.tolist()\n",
    "salary = salary.tolist()\n",
    "fn = [f/1000 for f in fnl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final log likelihood = -8329.14535628004\n",
      "Accuracy of predictor in train data =  0.796744471744\n",
      "Accuracy of predictor in valid data =  0.799447174447\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "from math import exp\n",
    "from math import log\n",
    "import numpy\n",
    "\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not(y[i]==True):\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    #print (\"ll =\", loglikelihood)\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for k in range(len(theta)): \n",
    "        for i in range(len(X)):\n",
    "            logit = inner(X[i], theta)\n",
    "            dl[k] += X[i][k]*(exp(-logit))/(1 + exp(-logit))\n",
    "            if not(y[i]==True):\n",
    "                dl[k] -= X[i][k]\n",
    "        dl[k] -= 2*lam*theta[k]\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])  \n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(capacity):\n",
    "    X.append([age[i],eduNum[i],capGain[i],capLoss[i],hpw[i],fn[i],sex[i]==' Male'])\n",
    "    Y.append(salary[i] == ' >50K')\n",
    "\n",
    "scale_half = int(capacity/2)\n",
    "X_train = X[:scale_half]\n",
    "Y_train = Y[:scale_half]\n",
    "X_valid = X[scale_half:]\n",
    "Y_valid = Y[scale_half:]\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, Y_train, 1.0))\n",
    "print \"Final log likelihood =\", -l\n",
    "\n",
    "YPre_train = []\n",
    "for i in range(scale_half):\n",
    "    x_theta = 0\n",
    "    for j in range(len(theta)):\n",
    "        x_theta += X_train[i][j]*theta[j]\n",
    "    if x_theta > 0:\n",
    "        YPre_train.append(1)\n",
    "    else:\n",
    "        YPre_train.append(0)\n",
    "count = 0\n",
    "for i in range(scale_half):\n",
    "    if Y_train[i] == YPre_train[i]:\n",
    "        count = count + 1\n",
    "acc = count/float(scale_half)\n",
    "print \"Accuracy of predictor in train data = \", acc\n",
    "\n",
    "YPre_valid = []\n",
    "for i in range(scale_half):\n",
    "    x_theta = 0\n",
    "    for j in range(len(theta)):\n",
    "        x_theta += X_valid[i][j]*theta[j]\n",
    "    if x_theta > 0:\n",
    "        YPre_valid.append(1)\n",
    "    else:\n",
    "        YPre_valid.append(0)\n",
    "count = 0\n",
    "for i in range(scale_half):\n",
    "    if Y_valid[i] == YPre_valid[i]:\n",
    "        count = count + 1\n",
    "acc = count/float(scale_half)\n",
    "print \"Accuracy of predictor in valid data = \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0 coefficient vector theta:  [-6.35349623e-03 -1.50877962e-02  3.20492440e-04  7.73964831e-04\n",
      " -1.48217915e-02 -3.85340929e-03  5.72306857e-01]\n",
      "lambda =  0 Accuracy of predictor in train data =  0.796805896806\n",
      "lambda =  0 Accuracy of predictor in valid data =  0.799385749386\n",
      "lambda =  0.01 coefficient vector theta:  [-6.35237566e-03 -1.50825774e-02  3.20490729e-04  7.74022304e-04\n",
      " -1.48224820e-02 -3.85363078e-03  5.72214665e-01]\n",
      "lambda =  0.01 Accuracy of predictor in train data =  0.796805896806\n",
      "lambda =  0.01 Accuracy of predictor in valid data =  0.799385749386\n",
      "lambda =  0.1 coefficient vector theta:  [-6.35346976e-03 -1.50777664e-02  3.20493825e-04  7.74106621e-04\n",
      " -1.48186682e-02 -3.85413412e-03  5.72065511e-01]\n",
      "lambda =  0.1 Accuracy of predictor in train data =  0.796805896806\n",
      "lambda =  0.1 Accuracy of predictor in valid data =  0.799385749386\n",
      "lambda =  1 coefficient vector theta:  [-6.35032827e-03 -1.50748478e-02  3.20522006e-04  7.73677630e-04\n",
      " -1.47905294e-02 -3.85182770e-03  5.69800551e-01]\n",
      "lambda =  1 Accuracy of predictor in train data =  0.796744471744\n",
      "lambda =  1 Accuracy of predictor in valid data =  0.799447174447\n",
      "lambda =  10 coefficient vector theta:  [-6.27019415e-03 -1.49447435e-02  3.20439672e-04  7.74021801e-04\n",
      " -1.46054240e-02 -3.84336882e-03  5.51256076e-01]\n",
      "lambda =  10 Accuracy of predictor in train data =  0.796621621622\n",
      "lambda =  10 Accuracy of predictor in valid data =  0.799508599509\n",
      "lambda =  100 coefficient vector theta:  [-5.74611325e-03 -1.40504516e-02  3.20347413e-04  7.75106149e-04\n",
      " -1.32135298e-02 -3.78167112e-03  4.15874468e-01]\n",
      "lambda =  100 Accuracy of predictor in train data =  0.796253071253\n",
      "lambda =  100 Accuracy of predictor in valid data =  0.8\n",
      "lambda =  1000 coefficient vector theta:  [-0.00468593 -0.01188812  0.00032163  0.00078137 -0.0102662  -0.00366947\n",
      "  0.12291351]\n",
      "lambda =  1000 Accuracy of predictor in train data =  0.797297297297\n",
      "lambda =  1000 Accuracy of predictor in valid data =  0.799078624079\n",
      "lambda =  10000 coefficient vector theta:  [-0.00463408 -0.00768328  0.00032228  0.00078352 -0.00949239 -0.00367936\n",
      "  0.01525443]\n",
      "lambda =  10000 Accuracy of predictor in train data =  0.797481572482\n",
      "lambda =  10000 Accuracy of predictor in valid data =  0.799078624079\n",
      "lambda =  100000 coefficient vector theta:  [-0.00521095 -0.00288265  0.00032062  0.00077549 -0.00873558 -0.00387759\n",
      "  0.00141354]\n",
      "lambda =  100000 Accuracy of predictor in train data =  0.797542997543\n",
      "lambda =  100000 Accuracy of predictor in valid data =  0.798771498771\n",
      "lambda =  1000000 coefficient vector theta:  [-4.09532402e-03 -1.14603432e-03  3.10507558e-04  7.33019957e-04\n",
      " -5.00352626e-03 -4.70134366e-03  7.25020286e-05]\n",
      "lambda =  1000000 Accuracy of predictor in train data =  0.796683046683\n",
      "lambda =  1000000 Accuracy of predictor in valid data =  0.79828009828\n"
     ]
    }
   ],
   "source": [
    "# Accuracy with lambda\n",
    "for lam in [0,0.01,0.1,1,10,100,1000,10000,100000,1000000]:\n",
    "    theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, Y_train, lam))\n",
    "    #print \"Final log likelihood =\", -l\n",
    "    print \"lambda = \",lam, \"coefficient vector theta: \",theta\n",
    "\n",
    "    YPre_train = []\n",
    "    for i in range(scale_half):\n",
    "        x_theta = 0\n",
    "        for j in range(len(theta)):\n",
    "            x_theta += X_train[i][j]*theta[j]\n",
    "        if x_theta > 0:\n",
    "            YPre_train.append(1)\n",
    "        else:\n",
    "            YPre_train.append(0)\n",
    "    count = 0\n",
    "    for i in range(scale_half):\n",
    "        if Y_train[i] == YPre_train[i]:\n",
    "            count = count + 1\n",
    "    acc = count/float(scale_half)\n",
    "    print \"lambda = \", lam, \"Accuracy of predictor in train data = \", acc\n",
    "\n",
    "    YPre_valid = []\n",
    "    for i in range(scale_half):\n",
    "        x_theta = 0\n",
    "        for j in range(len(theta)):\n",
    "            x_theta += X_valid[i][j]*theta[j]\n",
    "        if x_theta > 0:\n",
    "            YPre_valid.append(1)\n",
    "        else:\n",
    "            YPre_valid.append(0)\n",
    "    count = 0\n",
    "    for i in range(scale_half):\n",
    "        if Y_valid[i] == YPre_valid[i]:\n",
    "            count = count + 1\n",
    "    acc = count/float(scale_half)\n",
    "    print \"lambda = \", lam, \"Accuracy of predictor in valid data = \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test set:  16281\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data = pd.read_csv(\"adult.test.txt\",header=None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','salary-class']\n",
    "occ = data['occupation'].value_counts()\n",
    "wor = data['workclass'].value_counts()\n",
    "capacity = wor.sum()\n",
    "print \"size of test set: \", capacity\n",
    "\n",
    "# data extractionï¼š6 continuous features\n",
    "# age, education-num, capital-gain, capital-loss, hours-per-week,fnlwgt\n",
    "age = data['age']\n",
    "eduNum = data['education-num']\n",
    "capGain = data['capital-gain']\n",
    "capLoss = data['capital-loss']\n",
    "hpw = data['hours-per-week']\n",
    "fnl = data['fnlwgt']\n",
    "sex = data['sex']\n",
    "salary = data['salary-class']\n",
    "assert ' ?' not in age.value_counts().index  # no missing data\n",
    "assert ' ?' not in eduNum.value_counts().index\n",
    "assert ' ?' not in hpw.value_counts().index\n",
    "assert ' ?' not in capGain.value_counts().index\n",
    "assert ' ?' not in capLoss.value_counts().index\n",
    "assert ' ?' not in fnl.value_counts().index\n",
    "assert ' ?' not in sex.value_counts().index\n",
    "assert ' ?' not in salary.value_counts().index\n",
    "age = age.tolist()\n",
    "eduNum = eduNum.tolist()\n",
    "capGain = capGain.tolist()\n",
    "capLoss = capLoss.tolist()\n",
    "hpw = hpw.tolist()\n",
    "fnl = fnl.tolist()\n",
    "sex = sex.tolist()\n",
    "salary = salary.tolist()\n",
    "fn = [f/1000 for f in fnl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0 coefficient vector theta:  [-6.35349623e-03 -1.50877962e-02  3.20492440e-04  7.73964831e-04\n",
      " -1.48217915e-02 -3.85340929e-03  5.72306857e-01]\n",
      "lambda =  0 Accuracy of predictor in test data =  0.913887353357\n",
      "lambda =  0.01 coefficient vector theta:  [-6.35237566e-03 -1.50825774e-02  3.20490729e-04  7.74022304e-04\n",
      " -1.48224820e-02 -3.85363078e-03  5.72214665e-01]\n",
      "lambda =  0.01 Accuracy of predictor in test data =  0.913887353357\n",
      "lambda =  0.1 coefficient vector theta:  [-6.35346976e-03 -1.50777664e-02  3.20493825e-04  7.74106621e-04\n",
      " -1.48186682e-02 -3.85413412e-03  5.72065511e-01]\n",
      "lambda =  0.1 Accuracy of predictor in test data =  0.913887353357\n",
      "lambda =  1 coefficient vector theta:  [-6.35032827e-03 -1.50748478e-02  3.20522006e-04  7.73677630e-04\n",
      " -1.47905294e-02 -3.85182770e-03  5.69800551e-01]\n",
      "lambda =  1 Accuracy of predictor in test data =  0.913948774645\n",
      "lambda =  10 coefficient vector theta:  [-6.27019415e-03 -1.49447435e-02  3.20439672e-04  7.74021801e-04\n",
      " -1.46054240e-02 -3.84336882e-03  5.51256076e-01]\n",
      "lambda =  10 Accuracy of predictor in test data =  0.914255881088\n",
      "lambda =  100 coefficient vector theta:  [-5.74611325e-03 -1.40504516e-02  3.20347413e-04  7.75106149e-04\n",
      " -1.32135298e-02 -3.78167112e-03  4.15874468e-01]\n",
      "lambda =  100 Accuracy of predictor in test data =  0.9141944598\n",
      "lambda =  1000 coefficient vector theta:  [-0.00468593 -0.01188812  0.00032163  0.00078137 -0.0102662  -0.00366947\n",
      "  0.12291351]\n",
      "lambda =  1000 Accuracy of predictor in test data =  0.914071617223\n",
      "lambda =  10000 coefficient vector theta:  [-0.00463408 -0.00768328  0.00032228  0.00078352 -0.00949239 -0.00367936\n",
      "  0.01525443]\n",
      "lambda =  10000 Accuracy of predictor in test data =  0.913703089491\n",
      "lambda =  100000 coefficient vector theta:  [-0.00521095 -0.00288265  0.00032062  0.00077549 -0.00873558 -0.00387759\n",
      "  0.00141354]\n",
      "lambda =  100000 Accuracy of predictor in test data =  0.913825932068\n",
      "lambda =  1000000 coefficient vector theta:  [-4.09532402e-03 -1.14603432e-03  3.10507558e-04  7.33019957e-04\n",
      " -5.00352626e-03 -4.70134366e-03  7.25020286e-05]\n",
      "lambda =  1000000 Accuracy of predictor in test data =  0.913580246914\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "from math import exp\n",
    "from math import log\n",
    "import numpy\n",
    "\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not(y[i]==True):\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    #print (\"ll =\", loglikelihood)\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for k in range(len(theta)): \n",
    "        for i in range(len(X)):\n",
    "            logit = inner(X[i], theta)\n",
    "            dl[k] += X[i][k]*(exp(-logit))/(1 + exp(-logit))\n",
    "            if not(y[i]==True):\n",
    "                dl[k] -= X[i][k]\n",
    "        dl[k] -= 2*lam*theta[k]\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])  \n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(capacity):\n",
    "    X_test.append([age[i],eduNum[i],capGain[i],capLoss[i],hpw[i],fn[i],sex[i]==' Male'])\n",
    "    Y_test.append(salary[i] == ' >50K')\n",
    "    \n",
    "for lam in [0,0.01,0.1,1,10,100,1000,10000,100000,1000000]:\n",
    "    theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, Y_train, lam))\n",
    "    #print \"Final log likelihood =\", -l\n",
    "    print \"lambda = \",lam, \"coefficient vector theta: \",theta\n",
    "\n",
    "    YPre_test = []\n",
    "    for i in range(capacity):\n",
    "        x_theta = 0\n",
    "        for j in range(len(theta)):\n",
    "            x_theta += X_test[i][j]*theta[j]\n",
    "        if x_theta > 0:\n",
    "            YPre_test.append(1)\n",
    "        else:\n",
    "            YPre_test.append(0)\n",
    "    count = 0\n",
    "    for i in range(capacity):\n",
    "        if Y_test[i] == YPre_test[i]:\n",
    "            count = count + 1\n",
    "    acc = count/float(capacity)\n",
    "    print \"lambda = \", lam, \"Accuracy of predictor in test data = \", acc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
