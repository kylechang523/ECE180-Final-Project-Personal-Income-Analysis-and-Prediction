{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",header=None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','salary-class']\n",
    "occ = data['occupation'].value_counts()\n",
    "wor = data['workclass'].value_counts()\n",
    "capacity = wor.sum() #32561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of age is 13.651859975568133\n",
      "Standard deviation of eduNum is 2.5550951067314753\n",
      "Standard deviation of capGain is 7314.405625313393\n",
      "Standard deviation of capLoss is 401.25526866261686\n",
      "Standard deviation of hpw is 12.289313389422963\n",
      "C= 0.1 Accuracy of the predictor on the train data is: 0.760626535627\n",
      "C= 0.1 Accuracy of the predictor on the valid data is: 0.757800982801\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "# data extractionï¼š5 continuous features\n",
    "# age, education-num, capital-gain, capital-loss, hours-per-week\n",
    "age = data['age']\n",
    "eduNum = data['education-num']\n",
    "capGain = data['capital-gain']\n",
    "capLoss = data['capital-loss']\n",
    "hpw = data['hours-per-week']\n",
    "salary = data['salary-class']\n",
    "assert ' ?' not in age.value_counts().index  # no missing data\n",
    "assert ' ?' not in eduNum.value_counts().index\n",
    "assert ' ?' not in hpw.value_counts().index\n",
    "assert ' ?' not in capGain.value_counts().index\n",
    "assert ' ?' not in capLoss.value_counts().index\n",
    "assert ' ?' not in salary.value_counts().index\n",
    "age = age.tolist()\n",
    "eduNum = eduNum.tolist()\n",
    "capGain = capGain.tolist()\n",
    "capLoss = capLoss.tolist()\n",
    "hpw = hpw.tolist()\n",
    "salary = salary.tolist()\n",
    "scale_half = int(capacity/2)\n",
    "\n",
    "age_std = np.std(np.array(age[:scale_half]))\n",
    "eduNum_std = np.std(np.array(eduNum[:scale_half]))\n",
    "capGain_std = np.std(np.array(capGain[:scale_half]))\n",
    "capLoss_std = np.std(np.array(capLoss[:scale_half]))\n",
    "hpw_std = np.std(np.array(hpw[:scale_half]))\n",
    "print \"Standard deviation of age is\", age_std\n",
    "print \"Standard deviation of eduNum is\", eduNum_std\n",
    "print \"Standard deviation of capGain is\", capGain_std\n",
    "print \"Standard deviation of capLoss is\", capLoss_std\n",
    "print \"Standard deviation of hpw is\", hpw_std\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(capacity):\n",
    "    X.append([age[i],eduNum[i],hpw[i]])\n",
    "    Y.append(salary[i] == ' >50K')\n",
    "    \n",
    "# SVM Classification\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "X_train = X[:scale_half]\n",
    "Y_train = Y[:scale_half]\n",
    "X_valid = X[scale_half:]\n",
    "Y_valid = Y[scale_half:]\n",
    "\n",
    "c = 0.1\n",
    "clf = svm.SVC(C=c, kernel='linear')\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_predictions = clf.predict(X_train)\n",
    "valid_predictions = clf.predict(X_valid)\n",
    "\n",
    "train_correct = 0\n",
    "for i in range(scale_half):\n",
    "    if train_predictions[i] == Y_train[i]:\n",
    "        train_correct = train_correct + 1\n",
    "train_acc = train_correct/float(scale_half)\n",
    "valid_correct = 0\n",
    "for i in range(scale_half):\n",
    "    if valid_predictions[i] == Y_valid[i]:\n",
    "        valid_correct = valid_correct + 1\n",
    "valid_acc = valid_correct/float(scale_half)\n",
    "print \"C=\",c,\"Accuracy of the predictor on the train data is:\",train_acc\n",
    "print \"C=\",c,\"Accuracy of the predictor on the valid data is:\",valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final log likelihood = -8613.946116166113\n",
      "Accuracy of predictor in train data =  0.797235872236\n",
      "Accuracy of predictor in valid data =  0.797604422604\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "from math import exp\n",
    "from math import log\n",
    "import numpy\n",
    "\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not(y[i]==True):\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    #print (\"ll =\", loglikelihood)\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for k in range(len(theta)): \n",
    "        for i in range(len(X)):\n",
    "            logit = inner(X[i], theta)\n",
    "            dl[k] += X[i][k]*(exp(-logit))/(1 + exp(-logit))\n",
    "            if not(y[i]==True):\n",
    "                dl[k] -= X[i][k]\n",
    "        dl[k] -= 2*lam*theta[k]\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])  \n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(capacity):\n",
    "    X.append([age[i],eduNum[i],capGain[i],capLoss[i],hpw[i]])\n",
    "    Y.append(salary[i] == ' >50K')\n",
    "\n",
    "scale_half = int(capacity/2)\n",
    "X_train = X[:scale_half]\n",
    "Y_train = Y[:scale_half]\n",
    "X_valid = X[scale_half:]\n",
    "Y_valid = Y[scale_half:]\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, Y_train, 1.0))\n",
    "print \"Final log likelihood =\", -l\n",
    "\n",
    "YPre_train = []\n",
    "for i in range(scale_half):\n",
    "    x_theta = 0\n",
    "    for j in range(len(theta)):\n",
    "        x_theta += X_train[i][j]*theta[j]\n",
    "    if x_theta > 0:\n",
    "        YPre_train.append(1)\n",
    "    else:\n",
    "        YPre_train.append(0)\n",
    "count = 0\n",
    "for i in range(scale_half):\n",
    "    if Y_train[i] == YPre_train[i]:\n",
    "        count = count + 1\n",
    "acc = count/float(scale_half)\n",
    "print \"Accuracy of predictor in train data = \", acc\n",
    "\n",
    "YPre_valid = []\n",
    "for i in range(scale_half):\n",
    "    x_theta = 0\n",
    "    for j in range(len(theta)):\n",
    "        x_theta += X_valid[i][j]*theta[j]\n",
    "    if x_theta > 0:\n",
    "        YPre_valid.append(1)\n",
    "    else:\n",
    "        YPre_valid.append(0)\n",
    "count = 0\n",
    "for i in range(scale_half):\n",
    "    if Y_valid[i] == YPre_valid[i]:\n",
    "        count = count + 1\n",
    "acc = count/float(scale_half)\n",
    "print \"Accuracy of predictor in valid data = \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  0 coefficient vector theta:  [-0.00718288 -0.0422981   0.0003254   0.00080449 -0.01441198]\n",
      "lambda =  0 Accuracy of predictor in train data =  0.797235872236\n",
      "lambda =  0 Accuracy of predictor in valid data =  0.797604422604\n",
      "lambda =  0.01 coefficient vector theta:  [-0.00718289 -0.04229807  0.0003254   0.00080449 -0.01441198]\n",
      "lambda =  0.01 Accuracy of predictor in train data =  0.797235872236\n",
      "lambda =  0.01 Accuracy of predictor in valid data =  0.797604422604\n",
      "lambda =  0.1 coefficient vector theta:  [-0.00718291 -0.04229781  0.0003254   0.00080449 -0.01441202]\n",
      "lambda =  0.1 Accuracy of predictor in train data =  0.797235872236\n",
      "lambda =  0.1 Accuracy of predictor in valid data =  0.797604422604\n",
      "lambda =  1 coefficient vector theta:  [-0.00718313 -0.04229529  0.0003254   0.00080449 -0.01441239]\n",
      "lambda =  1 Accuracy of predictor in train data =  0.797235872236\n",
      "lambda =  1 Accuracy of predictor in valid data =  0.797604422604\n",
      "lambda =  10 coefficient vector theta:  [-0.00718588 -0.04227433  0.00032539  0.00080445 -0.0144141 ]\n",
      "lambda =  10 Accuracy of predictor in train data =  0.797235872236\n",
      "lambda =  10 Accuracy of predictor in valid data =  0.797604422604\n",
      "lambda =  100 coefficient vector theta:  [-0.00720956 -0.04203687  0.00032538  0.00080438 -0.01444753]\n",
      "lambda =  100 Accuracy of predictor in train data =  0.797235872236\n",
      "lambda =  100 Accuracy of predictor in valid data =  0.797665847666\n",
      "lambda =  1000 coefficient vector theta:  [-0.00741899 -0.03982736  0.00032517  0.00080357 -0.01476651]\n",
      "lambda =  1000 Accuracy of predictor in train data =  0.797174447174\n",
      "lambda =  1000 Accuracy of predictor in valid data =  0.79785012285\n",
      "lambda =  10000 coefficient vector theta:  [-0.00873725 -0.02669507  0.0003239   0.00079823 -0.0165783 ]\n",
      "lambda =  10000 Accuracy of predictor in train data =  0.79742014742\n",
      "lambda =  10000 Accuracy of predictor in valid data =  0.798218673219\n",
      "lambda =  100000 coefficient vector theta:  [-0.01126595 -0.00878026  0.00031871  0.00077781 -0.01777225]\n",
      "lambda =  100000 Accuracy of predictor in train data =  0.797665847666\n",
      "lambda =  100000 Accuracy of predictor in valid data =  0.798464373464\n",
      "lambda =  1000000 coefficient vector theta:  [-0.01142113 -0.00370803  0.0002849   0.00065369 -0.01354279]\n",
      "lambda =  1000000 Accuracy of predictor in train data =  0.79613022113\n",
      "lambda =  1000000 Accuracy of predictor in valid data =  0.796928746929\n"
     ]
    }
   ],
   "source": [
    "# Accuracy with lambda\n",
    "for lam in [0,0.01,0.1,1,10,100,1000,10000,100000,1000000]:\n",
    "    theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, Y_train, lam))\n",
    "    #print \"Final log likelihood =\", -l\n",
    "    print \"lambda = \",lam, \"coefficient vector theta: \",theta\n",
    "    \n",
    "    YPre_train = []\n",
    "    for i in range(scale_half):\n",
    "        x_theta = 0\n",
    "        for j in range(len(theta)):\n",
    "            x_theta += X_train[i][j]*theta[j]\n",
    "        if x_theta > 0:\n",
    "            YPre_train.append(1)\n",
    "        else:\n",
    "            YPre_train.append(0)\n",
    "    count = 0\n",
    "    for i in range(scale_half):\n",
    "        if Y_train[i] == YPre_train[i]:\n",
    "            count = count + 1\n",
    "    acc = count/float(scale_half)\n",
    "    print \"lambda = \", lam, \"Accuracy of predictor in train data = \", acc\n",
    "\n",
    "    YPre_valid = []\n",
    "    for i in range(scale_half):\n",
    "        x_theta = 0\n",
    "        for j in range(len(theta)):\n",
    "            x_theta += X_valid[i][j]*theta[j]\n",
    "        if x_theta > 0:\n",
    "            YPre_valid.append(1)\n",
    "        else:\n",
    "            YPre_valid.append(0)\n",
    "    count = 0\n",
    "    for i in range(scale_half):\n",
    "        if Y_valid[i] == YPre_valid[i]:\n",
    "            count = count + 1\n",
    "    acc = count/float(scale_half)\n",
    "    print \"lambda = \", lam, \"Accuracy of predictor in valid data = \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1.0:\taccuracy of training set = 0.7972358722358722\n",
      "lambda = 1.0:\taccuracy of valid set = 0.797616854001597\n",
      "TP = 1051 TN = 11935 FP = 402 FN = 2893\n",
      "BER of classifier on valid set is 0.149532818706\n"
     ]
    }
   ],
   "source": [
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, Y_train, lam))           \n",
    "    return theta\n",
    "\n",
    "def performance(theta, X, y):\n",
    "    scores = [inner(theta,x) for x in X]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    correct = [(a==b) for (a,b) in zip(predictions,y)]\n",
    "    acc = sum(correct) * 1.0 / len(correct)\n",
    "    return acc\n",
    "\n",
    "#BER Calculation\n",
    "def ber(theta, X, y):\n",
    "    scores = [inner(theta, x) for x in X]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    TP = [(a==1 and b==1) for (a,b) in zip(predictions, y)]\n",
    "    TN = [(a==0 and b==0) for (a,b) in zip(predictions, y)]\n",
    "    FP = [(a==1 and b==0) for (a,b) in zip(predictions, y)]\n",
    "    FN = [(a==0 and b==1) for (a,b) in zip(predictions, y)]\n",
    "    tp = sum(TP)\n",
    "    tn = sum(TN)\n",
    "    fp = sum(FP)\n",
    "    fn = sum(FN)\n",
    "    print(\"TP = \"+str(tp)+\" TN = \"+str(tn)+\" FP = \"+str(fp)+\" FN = \"+str(fn))\n",
    "    BER = 1-0.5*(float(fn)/(tp+fn)+float(tn)/(tn+fp))\n",
    "    return BER\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "Theta_1 = train(lam)\n",
    "acc1 = performance(Theta_1, X_train, Y_train)\n",
    "acc2 = performance(Theta_1, X_valid, Y_valid)\n",
    "\n",
    "print (\"lambda = \" + str(lam) + \":\\taccuracy of training set = \" + str(acc1))\n",
    "print (\"lambda = \" + str(lam) + \":\\taccuracy of valid set = \" + str(acc2))\n",
    "\n",
    "BER_1 = ber(Theta_1, X_valid, Y_valid)\n",
    "print(\"BER of classifier on valid set is \"+str(BER_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test set:  16281\n",
      "lambda =  0 coefficient vector theta:  [-0.00718288 -0.0422981   0.0003254   0.00080449 -0.01441198]\n",
      "lambda =  0 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  0.01 coefficient vector theta:  [-0.00718289 -0.04229807  0.0003254   0.00080449 -0.01441198]\n",
      "lambda =  0.01 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  0.1 coefficient vector theta:  [-0.00718291 -0.04229781  0.0003254   0.00080449 -0.01441202]\n",
      "lambda =  0.1 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  1 coefficient vector theta:  [-0.00718313 -0.04229529  0.0003254   0.00080449 -0.01441239]\n",
      "lambda =  1 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  10 coefficient vector theta:  [-0.00718588 -0.04227433  0.00032539  0.00080445 -0.0144141 ]\n",
      "lambda =  10 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  100 coefficient vector theta:  [-0.00720956 -0.04203687  0.00032538  0.00080438 -0.01444753]\n",
      "lambda =  100 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  1000 coefficient vector theta:  [-0.00741899 -0.03982736  0.00032517  0.00080357 -0.01476651]\n",
      "lambda =  1000 Accuracy of predictor in test data =  0.912658927584\n",
      "lambda =  10000 coefficient vector theta:  [-0.00873725 -0.02669507  0.0003239   0.00079823 -0.0165783 ]\n",
      "lambda =  10000 Accuracy of predictor in test data =  0.913027455316\n",
      "lambda =  100000 coefficient vector theta:  [-0.01126595 -0.00878026  0.00031871  0.00077781 -0.01777225]\n",
      "lambda =  100000 Accuracy of predictor in test data =  0.91284319145\n",
      "lambda =  1000000 coefficient vector theta:  [-0.01142113 -0.00370803  0.0002849   0.00065369 -0.01354279]\n",
      "lambda =  1000000 Accuracy of predictor in test data =  0.908605122535\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "data = pd.read_csv(\"adult.test.txt\",header=None)\n",
    "data.columns = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','salary-class']\n",
    "occ = data['occupation'].value_counts()\n",
    "wor = data['workclass'].value_counts()\n",
    "capacity = wor.sum() \n",
    "print \"size of test set: \", capacity\n",
    "# prediction\n",
    "# data extractionï¼š5 continuous features\n",
    "# age, education-num, capital-gain, capital-loss, hours-per-week\n",
    "age = data['age']\n",
    "eduNum = data['education-num']\n",
    "capGain = data['capital-gain']\n",
    "capLoss = data['capital-loss']\n",
    "hpw = data['hours-per-week']\n",
    "salary = data['salary-class']\n",
    "assert ' ?' not in age.value_counts().index  # no missing data\n",
    "assert ' ?' not in eduNum.value_counts().index\n",
    "assert ' ?' not in hpw.value_counts().index\n",
    "assert ' ?' not in capGain.value_counts().index\n",
    "assert ' ?' not in capLoss.value_counts().index\n",
    "assert ' ?' not in salary.value_counts().index\n",
    "age = age.tolist()\n",
    "eduNum = eduNum.tolist()\n",
    "capGain = capGain.tolist()\n",
    "capLoss = capLoss.tolist()\n",
    "hpw = hpw.tolist()\n",
    "salary = salary.tolist()\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(capacity):\n",
    "    X_test.append([age[i],eduNum[i],capGain[i],capLoss[i],hpw[i]])\n",
    "    Y_test.append(salary[i] == ' >50K')\n",
    "\n",
    "# Logistic Regression\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "from math import exp\n",
    "from math import log\n",
    "import numpy\n",
    "\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not(y[i]==True):\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    #print (\"ll =\", loglikelihood)\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for k in range(len(theta)): \n",
    "        for i in range(len(X)):\n",
    "            logit = inner(X[i], theta)\n",
    "            dl[k] += X[i][k]*(exp(-logit))/(1 + exp(-logit))\n",
    "            if not(y[i]==True):\n",
    "                dl[k] -= X[i][k]\n",
    "        dl[k] -= 2*lam*theta[k]\n",
    "  # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])  \n",
    "    \n",
    "for lam in [0,0.01,0.1,1,10,100,1000,10000,100000,1000000]:\n",
    "    theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, args = (X_train, Y_train, lam))\n",
    "    #print \"Final log likelihood =\", -l\n",
    "    print \"lambda = \",lam, \"coefficient vector theta: \",theta\n",
    "\n",
    "    YPre_test = []\n",
    "    for i in range(capacity):\n",
    "        x_theta = 0\n",
    "        for j in range(len(theta)):\n",
    "            x_theta += X_test[i][j]*theta[j]\n",
    "        if x_theta > 0:\n",
    "            YPre_test.append(1)\n",
    "        else:\n",
    "            YPre_test.append(0)\n",
    "    count = 0\n",
    "    for i in range(capacity):\n",
    "        if Y_test[i] == YPre_test[i]:\n",
    "            count = count + 1\n",
    "    acc = count/float(capacity)\n",
    "    print \"lambda = \", lam, \"Accuracy of predictor in test data = \", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
